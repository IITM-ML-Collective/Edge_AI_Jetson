{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87fdec28",
   "metadata": {},
   "source": [
    "## Vehicle detection training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e81cb078",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 640\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "MODEL = \"yolov5s\"\n",
    "WORKERS = 1\n",
    "PROJECT = \"vehicles_data_pyimagesearch\"\n",
    "RUN_NAME = f\"{MODEL}_size{SIZE}_epochs{EPOCHS}_batch{BATCH_SIZE}_small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d44c59a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=./vehicle_data/vehicle_data.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=5, batch_size=32, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=yolov5/data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=1, project=vehicles_data_pyimagesearch, name=yolov5s_size640_epochs5_batch32_small, exist_ok=True, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
      "YOLOv5 ðŸš€ v7.0-416-gfe1d4d99 Python-3.9.20 torch-1.13.1+cu117 CUDA:0 (NVIDIA RTX A6000, 48662MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir vehicles_data_pyimagesearch', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     26970  models.yolo.Detect                      [5, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7033114 parameters, 7033114 gradients, 16.0 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/fmlpc/app_project/vehicle_data/train/labels... 878 images,\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/fmlpc/app_project/vehicle_data/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/fmlpc/app_project/vehicle_data/valid/labels... 250 images, 0\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/fmlpc/app_project/vehicle_data/valid/labels.cache\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m3.82 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
      "Plotting labels to vehicles_data_pyimagesearch/yolov5s_size640_epochs5_batch32_small/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 1 dataloader workers\n",
      "Logging results to \u001b[1mvehicles_data_pyimagesearch/yolov5s_size640_epochs5_batch32_small\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/4       6.8G    0.09552    0.03587    0.04437         56        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        250        454      0.181      0.359      0.198     0.0912\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/4      8.42G    0.06306    0.02907    0.03091         47        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        250        454       0.29      0.439      0.307      0.149\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/4      8.42G    0.05752    0.02637    0.02557         62        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        250        454      0.279      0.536      0.344      0.137\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/4      8.42G     0.0527      0.025    0.02072         51        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        250        454      0.425      0.529       0.47      0.264\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/4      8.42G    0.05051    0.02389    0.01892         38        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        250        454      0.421      0.555      0.459      0.279\n",
      "\n",
      "5 epochs completed in 0.014 hours.\n",
      "Optimizer stripped from vehicles_data_pyimagesearch/yolov5s_size640_epochs5_batch32_small/weights/last.pt, 14.5MB\n",
      "Optimizer stripped from vehicles_data_pyimagesearch/yolov5s_size640_epochs5_batch32_small/weights/best.pt, 14.5MB\n",
      "\n",
      "Validating vehicles_data_pyimagesearch/yolov5s_size640_epochs5_batch32_small/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7023610 parameters, 0 gradients, 15.8 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        250        454      0.415      0.555      0.458      0.278\n",
      "             Ambulance        250         64       0.38      0.536      0.448      0.295\n",
      "                   Bus        250         46      0.407      0.609      0.572      0.398\n",
      "                   Car        250        238      0.286      0.563      0.411      0.233\n",
      "            Motorcycle        250         46      0.687      0.669       0.62      0.331\n",
      "                 Truck        250         60      0.312        0.4      0.237      0.132\n",
      "Results saved to \u001b[1mvehicles_data_pyimagesearch/yolov5s_size640_epochs5_batch32_small\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python yolov5/train.py --img {SIZE}\\\n",
    "               --batch {BATCH_SIZE}\\\n",
    "               --epochs {EPOCHS}\\\n",
    "               --data ./vehicle_data/vehicle_data.yaml\\\n",
    "               --weights {MODEL}.pt\\\n",
    "               --workers {WORKERS}\\\n",
    "               --project {PROJECT}\\\n",
    "               --name {RUN_NAME}\\\n",
    "               --exist-ok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d744ae",
   "metadata": {},
   "source": [
    "## License Plate Detection training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28f63c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 640\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "MODEL = \"yolov5s\"\n",
    "WORKERS = 1\n",
    "PROJECT = \"license_data_pyimagesearch\"\n",
    "RUN_NAME = f\"{MODEL}_size{SIZE}_epochs{EPOCHS}_batch{BATCH_SIZE}_small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee7afc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=./license_plate_data/plate_data.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=5, batch_size=32, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=yolov5/data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=1, project=license_data_pyimagesearch, name=yolov5s_size640_epochs5_batch32_small, exist_ok=True, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
      "YOLOv5 ðŸš€ v7.0-416-gfe1d4d99 Python-3.9.20 torch-1.13.1+cu117 CUDA:0 (NVIDIA RTX A6000, 48662MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir license_data_pyimagesearch', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/fmlpc/app_project/license_plate_data/train/labels... 21173\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/fmlpc/app_project/license_plate_data/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/fmlpc/app_project/license_plate_data/valid/labels... 2046 im\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/fmlpc/app_project/license_plate_data/valid/labels.cache\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.88 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
      "Plotting labels to license_data_pyimagesearch/yolov5s_size640_epochs5_batch32_small/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 1 dataloader workers\n",
      "Logging results to \u001b[1mlicense_data_pyimagesearch/yolov5s_size640_epochs5_batch32_small\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/4      6.81G    0.05499    0.01468          0         34        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2046       2132      0.952      0.887       0.94      0.545\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/4      8.43G    0.03516    0.00859          0         28        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2046       2132      0.956       0.94      0.968       0.56\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/4      8.43G    0.03165   0.007749          0         32        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2046       2132      0.952      0.936      0.968      0.601\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/4      8.43G    0.02798    0.00719          0         38        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2046       2132      0.967      0.941      0.974      0.604\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/4      8.43G    0.02689   0.007034          0         31        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2046       2132      0.956      0.954      0.977      0.647\n",
      "\n",
      "5 epochs completed in 0.240 hours.\n",
      "Optimizer stripped from license_data_pyimagesearch/yolov5s_size640_epochs5_batch32_small/weights/last.pt, 14.4MB\n",
      "Optimizer stripped from license_data_pyimagesearch/yolov5s_size640_epochs5_batch32_small/weights/best.pt, 14.4MB\n",
      "\n",
      "Validating license_data_pyimagesearch/yolov5s_size640_epochs5_batch32_small/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2046       2132      0.956      0.954      0.977      0.648\n",
      "Results saved to \u001b[1mlicense_data_pyimagesearch/yolov5s_size640_epochs5_batch32_small\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python yolov5/train.py --img {SIZE}\\\n",
    "               --batch {BATCH_SIZE}\\\n",
    "               --epochs {EPOCHS}\\\n",
    "               --data ./license_plate_data/plate_data.yaml\\\n",
    "               --weights {MODEL}.pt\\\n",
    "               --workers {WORKERS}\\\n",
    "               --project {PROJECT}\\\n",
    "               --name {RUN_NAME}\\\n",
    "               --exist-ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc4e6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flwr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
